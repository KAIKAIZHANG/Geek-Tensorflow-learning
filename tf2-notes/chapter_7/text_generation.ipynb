{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "sys.version_info(major=3, minor=6, micro=4, releaselevel='final', serial=0)\n",
      "matplotlib 2.1.2\n",
      "numpy 1.19.1\n",
      "pandas 0.22.0\n",
      "sklearn 0.19.1\n",
      "tensorflow 2.2.0\n",
      "tensorflow.keras 2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
    "input_filepath = \"./shakespeare.txt\"\n",
    "text = open(input_filepath, 'r').read()\n",
    "\n",
    "print(len(text))\n",
    "print(text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# 1. generate vocab\n",
    "# 2. build mapping char->id\n",
    "# 3. data -> id_data\n",
    "# 4. abcd -> bcd<eos> 输入输出\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "print(len(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "char_2idx = {char:idx for idx, char in enumerate(vocab)}\n",
    "print(char_2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
      " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
      " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
      " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n",
      "['\\n' ' ' '!']\n",
      "['\\n' ' ' '!']\n"
     ]
    }
   ],
   "source": [
    "idx2char = np.array(vocab)\n",
    "print(idx2char)\n",
    "# 通过id转换（获取）字符\n",
    "print(idx2char[[0,1,2]])\n",
    "print(idx2char[np.array([0,1,2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 47 56 57 58  1 15 47 58 47]\n",
      "First Citi\n"
     ]
    }
   ],
   "source": [
    "text_as_int = np.array([char_2idx[c] for c in text])\n",
    "print(text_as_int[0:10])\n",
    "print(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(18, shape=(), dtype=int64) F\n",
      "tf.Tensor(47, shape=(), dtype=int64) i\n",
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59  1], shape=(101,), dtype=int64)\n",
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "tf.Tensor(\n",
      "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
      " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
      " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
      "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
      " 63 53 59  1 49], shape=(101,), dtype=int64)\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(id_text):\n",
    "    \"\"\"\n",
    "    abcde -> abcd, cde\n",
    "    \"\"\"\n",
    "    return id_text[0:-1], id_text[1:]\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)  # 这里的dataset是字符级的，下面需要进行转换，转换为句子级的。\n",
    "seq_length = 100\n",
    "# 将dataset转变为sequence的dataset\n",
    "# 这里加1是因为，对于输入长度为5的，做完切分之后，长度都会变为4，长度会减1，所以如果想要100的长度，提前先加1\n",
    "# drop_remainder = True，最后的一个batch长度如果不够，那就丢弃\n",
    "seq_dataset = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for ch_id in char_dataset.take(2):\n",
    "    print(ch_id, idx2char[ch_id.numpy()])\n",
    "    \n",
    "for seq_id in seq_dataset.take(2):\n",
    "    print(seq_id)\n",
    "    print(repr(''.join(idx2char[seq_id.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59]\n",
      "[47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43  1\n",
      " 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43 39\n",
      " 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49  6\n",
      "  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0\n",
      " 37 53 59  1]\n",
      "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
      " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
      " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
      "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
      " 63 53 59  1]\n",
      "[56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1 58\n",
      " 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0 13\n",
      " 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8  0\n",
      "  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1 63\n",
      " 53 59  1 49]\n"
     ]
    }
   ],
   "source": [
    "seq_dataset = seq_dataset.map(split_input_target)\n",
    "\n",
    "for item_input, item_output in seq_dataset.take(2):\n",
    "    print(item_input.numpy())\n",
    "    print(item_output.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 10000\n",
    "\n",
    "seq_dataset = seq_dataset.shuffle(buffer_size).batch(\n",
    "    batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (64, None, 1024)          1311744   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 1,395,009\n",
      "Trainable params: 1,395,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256 # 词表比较小，所以设置的维度大一些\n",
    "rnn_units = 1024\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape = [batch_size, None]),\n",
    "        keras.layers.SimpleRNN(units=rnn_units,\n",
    "                              stateful=True,\n",
    "                              recurrent_initializer='glorot_uniform',\n",
    "                              return_sequences=True),  # 输出预测也是一个序列\n",
    "        keras.layers.Dense(vocab_size), # 模型最后一层输出没有激活函数，所以就是logits类型的输出\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in seq_dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "64：batch_size, 100:句子长度，65：预测类别个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[12]\n",
      " [53]\n",
      " [53]\n",
      " [23]\n",
      " [56]\n",
      " [17]\n",
      " [17]\n",
      " [25]\n",
      " [30]\n",
      " [13]\n",
      " [37]\n",
      " [23]\n",
      " [15]\n",
      " [10]\n",
      " [45]\n",
      " [16]\n",
      " [42]\n",
      " [16]\n",
      " [46]\n",
      " [15]\n",
      " [19]\n",
      " [35]\n",
      " [24]\n",
      " [33]\n",
      " [13]\n",
      " [53]\n",
      " [54]\n",
      " [59]\n",
      " [21]\n",
      " [12]\n",
      " [32]\n",
      " [31]\n",
      " [47]\n",
      " [64]\n",
      " [49]\n",
      " [36]\n",
      " [36]\n",
      " [30]\n",
      " [33]\n",
      " [40]\n",
      " [53]\n",
      " [20]\n",
      " [51]\n",
      " [14]\n",
      " [39]\n",
      " [29]\n",
      " [ 2]\n",
      " [45]\n",
      " [31]\n",
      " [20]\n",
      " [13]\n",
      " [ 6]\n",
      " [38]\n",
      " [64]\n",
      " [11]\n",
      " [26]\n",
      " [47]\n",
      " [ 4]\n",
      " [20]\n",
      " [34]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [15]\n",
      " [39]\n",
      " [24]\n",
      " [ 4]\n",
      " [15]\n",
      " [ 5]\n",
      " [33]\n",
      " [60]\n",
      " [51]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [44]\n",
      " [10]\n",
      " [26]\n",
      " [60]\n",
      " [39]\n",
      " [38]\n",
      " [39]\n",
      " [26]\n",
      " [37]\n",
      " [21]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [34]\n",
      " [18]\n",
      " [22]\n",
      " [46]\n",
      " [10]\n",
      " [61]\n",
      " [37]\n",
      " [25]\n",
      " [16]\n",
      " [32]\n",
      " [61]\n",
      " [62]\n",
      " [50]\n",
      " [46]\n",
      " [ 9]], shape=(100, 1), dtype=int64)\n",
      "tf.Tensor(\n",
      "[12 53 53 23 56 17 17 25 30 13 37 23 15 10 45 16 42 16 46 15 19 35 24 33\n",
      " 13 53 54 59 21 12 32 31 47 64 49 36 36 30 33 40 53 20 51 14 39 29  2 45\n",
      " 31 20 13  6 38 64 11 26 47  4 20 34  2  3 15 39 24  4 15  5 33 60 51  2\n",
      "  4 44 10 26 60 39 38 39 26 37 21  4  0 34 18 22 46 10 61 37 25 16 32 61\n",
      " 62 50 46  9], shape=(100,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# random sampling.\n",
    "# 通过随机采样的方式，基于上面的概率分布来生成一段话。\n",
    "# greedy（谈心策略，直接使用概率）, random（随机策略）.基于谈心算法最终只能生成一个序列，而采用随机采样算法，可以生成得到多个序列\n",
    "sample_indices = tf.random.categorical(\n",
    "    logits=example_batch_predictions[0], num_samples=1) # 采样个数num_samples = 1，也可以设置多个，这样就可以采样多个序列\n",
    "print(sample_indices)\n",
    "# (100, 65) -> (100, 1)，对100中的每一个位置都去做一个sample，最终一个位置从65个类别中采样得到一个结果\n",
    "sample_indices = tf.squeeze(sample_indices, axis=-1)\n",
    "print(sample_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  't sea?\\n\\nISABELLA:\\nI have heard of the lady, and good words went with her name.\\n\\nDUKE VINCENTIO:\\nShe '\n",
      "\n",
      "Output:  ' sea?\\n\\nISABELLA:\\nI have heard of the lady, and good words went with her name.\\n\\nDUKE VINCENTIO:\\nShe s'\n",
      "\n",
      "Predictions:  \"?ooKrEEMRAYKC:gDdDhCGWLUAopuI?TSizkXXRUboHmBaQ!gSHA,Zz;Ni&HV!$CaL&C'Uvm!&f:NvaZaNYI&\\nVFJh:wYMDTwxlh3\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Output: \", repr(\"\".join(idx2char[target_example_batch[0]])))\n",
    "print()\n",
    "print(\"Predictions: \", repr(\"\".join(idx2char[sample_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100)\n",
      "4.184099\n"
     ]
    }
   ],
   "source": [
    "# 定义模型的损失函数\n",
    "# 之前的损失函数直接是在激活函数之后使用的字符串指定\"sparse_categorical_crossentropy\"，而这里我们没有加激活函数，所以这里设置from_logits=True\n",
    "def loss(labels, logits):\n",
    "    return keras.losses.sparse_categorical_crossentropy(\n",
    "        labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=loss)\n",
    "example_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(example_loss.shape)\n",
    "print(example_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "172/172 [==============================] - 9s 52ms/step - loss: 3.0067\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 9s 51ms/step - loss: 2.2179\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 9s 51ms/step - loss: 1.9855\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.8369\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 9s 52ms/step - loss: 1.7279\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 9s 51ms/step - loss: 1.6503\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 9s 49ms/step - loss: 1.5885\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.5424\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.5049\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 9s 49ms/step - loss: 1.4753\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.4505\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.4264\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.4078\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3908\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3765\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3616\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3489\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3373\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3261\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3146\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3024\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2918\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2832\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2722\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2603\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2514\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2431\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2333\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2233\n",
      "Epoch 30/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2145\n",
      "Epoch 31/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2046\n",
      "Epoch 32/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1956\n",
      "Epoch 33/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1870\n",
      "Epoch 34/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1791\n",
      "Epoch 35/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.1682\n",
      "Epoch 36/100\n",
      "172/172 [==============================] - 9s 51ms/step - loss: 1.1597\n",
      "Epoch 37/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.1537\n",
      "Epoch 38/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1435\n",
      "Epoch 39/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.1351\n",
      "Epoch 40/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.1276\n",
      "Epoch 41/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.1212\n",
      "Epoch 42/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.1129\n",
      "Epoch 43/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.1060\n",
      "Epoch 44/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0980\n",
      "Epoch 45/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.0926\n",
      "Epoch 46/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0862\n",
      "Epoch 47/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0782\n",
      "Epoch 48/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0742\n",
      "Epoch 49/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0683\n",
      "Epoch 50/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.0642\n",
      "Epoch 51/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0566\n",
      "Epoch 52/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.0524\n",
      "Epoch 53/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0497\n",
      "Epoch 54/100\n",
      "172/172 [==============================] - 9s 49ms/step - loss: 1.0465\n",
      "Epoch 55/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0426\n",
      "Epoch 56/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0387\n",
      "Epoch 57/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0341\n",
      "Epoch 58/100\n",
      "172/172 [==============================] - 9s 51ms/step - loss: 1.0339\n",
      "Epoch 59/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0304\n",
      "Epoch 60/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.0288\n",
      "Epoch 61/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0265\n",
      "Epoch 62/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0251\n",
      "Epoch 63/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0221\n",
      "Epoch 64/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.0196\n",
      "Epoch 65/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.0190\n",
      "Epoch 66/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.0190\n",
      "Epoch 67/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0203\n",
      "Epoch 68/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.0166\n",
      "Epoch 69/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0165\n",
      "Epoch 70/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0155\n",
      "Epoch 71/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0165\n",
      "Epoch 72/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.0168\n",
      "Epoch 73/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0197\n",
      "Epoch 74/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0167\n",
      "Epoch 75/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0163\n",
      "Epoch 76/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0165\n",
      "Epoch 77/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.0178\n",
      "Epoch 78/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0188\n",
      "Epoch 79/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.0207\n",
      "Epoch 80/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0245\n",
      "Epoch 81/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0229\n",
      "Epoch 82/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.0247\n",
      "Epoch 83/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0258\n",
      "Epoch 84/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.0241\n",
      "Epoch 85/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0290\n",
      "Epoch 86/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0306\n",
      "Epoch 87/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.0333\n",
      "Epoch 88/100\n",
      "172/172 [==============================] - 9s 49ms/step - loss: 1.0323\n",
      "Epoch 89/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.0389\n",
      "Epoch 90/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0378\n",
      "Epoch 91/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0404\n",
      "Epoch 92/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0420\n",
      "Epoch 93/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0448\n",
      "Epoch 94/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0461\n",
      "Epoch 95/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0504\n",
      "Epoch 96/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.0545\n",
      "Epoch 97/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0530\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0604\n",
      "Epoch 99/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.0558\n",
      "Epoch 100/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.0624\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./text_generation_checkpoints\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "checkpoint_prefix = os.path.join(output_dir, 'ckpt_{epoch}')\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "epochs = 100\n",
    "history = model.fit(seq_dataset, epochs=epochs, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./text_generation_checkpoints/ckpt_100'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (1, None, 1024)           1311744   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 1,395,009\n",
      "Trainable params: 1,395,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 从checkpoint载入模型\n",
    "model2 = build_model(vocab_size,\n",
    "                     embedding_dim,\n",
    "                     rnn_units,\n",
    "                     batch_size=1) # 一次只生成一个句子\n",
    "model2.load_weights(tf.train.latest_checkpoint(output_dir))\n",
    "model2.build(tf.TensorShape([1, None])) # 一个变长输入\n",
    "# 大写表示序列，小写表示字符\n",
    "# start ch sequence A,\n",
    "# A -> model -> b\n",
    "# A.append(b) -> B\n",
    "# B(Ab) -> model -> c\n",
    "# B.append(c) -> C\n",
    "# C(Abc) -> model -> ...\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: I must confess so long amazed me! this one people years. Through enr of my wordward.\n",
      "Gentlemen, too much alone.\n",
      "\n",
      "GREGORIO:\n",
      "Bear with minument must be exceed\n",
      "Was slave my weakness that we died,\n",
      "He stay before his head well please?\n",
      "\n",
      "CORIOLANUS:\n",
      "Mispeak of love!\n",
      "But, sir, I with tears; for all of us offended t butt, and from remains\n",
      "And look'd for himself perforce quarrel half our glorious Tybalt marr'd and ambroker tears are come; in purposedate so proper babe.\n",
      "And there was book how wears from Rome again\n",
      "Without counteoushis wars and the sea of me.\n",
      "\n",
      "PAULINA:\n",
      "Why, wherefore? sore! loss have of my bounself of me;\n",
      "For I have been consider thee.\n",
      "\n",
      "Gag it, the quoch being hold his pamb my,\n",
      "And bear their torments than bound to visit you not ontend.\n",
      "\n",
      "Provost:\n",
      "I told you, Ifore\n",
      "As to be so boldness of my king by me, as yours, when man an' unrayor this be sworn breed.\n",
      "Stand, ere thou wast thine,\n",
      "Who hadst as gone, sir?\n",
      "\n",
      "COMINIUS:\n",
      "If it be so, the old more strength,\n",
      "Which within your requess.\n",
      "\n",
      "PO\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string, num_generate=1000):\n",
    "    input_eval = [char_2idx[ch] for ch in start_string] # 字符转为id序列，维度为1\n",
    "    input_eval = tf.expand_dims(input_eval, 0) # 进行维度扩展，[1, None]\n",
    "    \n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "    \n",
    "    for _ in range(num_generate):\n",
    "        # 1. model inference -> predictions\n",
    "        # 2. sample ->   ch -> text_generated.\n",
    "        # 3. update input_eval\n",
    "        \n",
    "        # predictions : [batch_size, input_eval_len, vocab_size]\n",
    "        predictions = model(input_eval)\n",
    "        # 将三维的预测结果转为二维，直接将batch_size=1消掉\n",
    "        # predictions : [input_eval_len, vocab_size]\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # predicted_ids: [input_eval_len, 1]\n",
    "        # a b c -> b c d\n",
    "        predicted_id = tf.random.categorical(\n",
    "            predictions, num_samples=1)[-1, 0].numpy() # 只取预测出的结果，所以取最后一个（ch），取出来加入到text_generated\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        # s,x -> rnn -> s',y\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "new_text = generate_text(model2, \"All: \")\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
